{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE for Breast Cancer Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples: 569\n",
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Import dataset\n",
    "breast_cancer_dataset = load_breast_cancer()\n",
    "data = breast_cancer_dataset['data']\n",
    "labels = breast_cancer_dataset['target']\n",
    "target_names = breast_cancer_dataset['target_names']\n",
    "feature_names = breast_cancer_dataset['feature_names']\n",
    "\n",
    "# Split test/train\n",
    "data_train = data[:500,]\n",
    "labels_train = labels[:500,]\n",
    "data_test = data[500:,]\n",
    "labels_test = labels[500:,]\n",
    "\n",
    "# Print out key stats\n",
    "print(f'Number of data samples: {data.shape[0]}')\n",
    "print(f'Number of features: {len(feature_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import dense\n",
    "import numpy as np\n",
    "\n",
    "seed = 11\n",
    "\n",
    "def accuracy(guesses, labels):\n",
    "    return np.mean([g == l for g, l in zip(guesses, labels)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Supervised Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_super = tf.Graph()\n",
    "with g_super.as_default():\n",
    "    \n",
    "    # Set tf seed\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    # Inputs\n",
    "    x_super = tf.placeholder(tf.float32, shape=[None, 30], name='x')\n",
    "    y_super = tf.placeholder(tf.int32, shape=[None,], name='y')\n",
    "\n",
    "    # Model\n",
    "    logits = dense(dense(inputs=x_super, activation='relu', units=30), activation=None, units=2)\n",
    "    y_hat_super = tf.argmax(logits, 1)\n",
    "\n",
    "    # Loss\n",
    "    cost = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=y_super)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "    # Summaries\n",
    "    tf.summary.scalar(\"Total_Loss\", cost)\n",
    "    merged_super = tf.summary.merge_all()\n",
    "    \n",
    "    # Saver\n",
    "    supervised_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Supervised Training on Supervised Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of supervised model on train set at 0 iterations: 0.592\n",
      "Accuracy of supervised model on train set at 500 iterations: 0.888\n",
      "Accuracy of supervised model on train set at 1000 iterations: 0.932\n",
      "Accuracy of supervised model on train set at 1500 iterations: 0.944\n",
      "Accuracy of supervised model on train set at 2000 iterations: 0.98\n",
      "Accuracy of supervised model on train set at 2500 iterations: 0.96\n",
      "Model saved in path: ./supervised/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "with tf.Session(graph=g_super) as sess:\n",
    "    # Initialize variables and saver and Tensorboard writer\n",
    "    writer = tf.summary.FileWriter('./supervised', g_super)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(2501):\n",
    "        # Generate training batches\n",
    "        indexes = np.random.randint(low=0, high=data_train.shape[0]-1, size=250)\n",
    "        feed_dict = {x_super: data_train[indexes], y_super: labels_train[indexes]}\n",
    "\n",
    "        # Training iteration\n",
    "        summary, y_hatt, _ = sess.run([merged_super, y_hat_super, optimizer], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(f'Accuracy of supervised model on train set at {step} iterations: {accuracy(y_hatt, labels[indexes])}')\n",
    "        writer.add_summary(summary=summary, global_step=step)\n",
    "    \n",
    "    # Save model weights\n",
    "    save_path = supervised_saver.save(sess, \"./supervised/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Trained Supervised Model on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./supervised/model.ckpt\n",
      "Accuracy of supervised model on test set: 0.9855072463768116\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g_super) as sess:\n",
    "    # Initialize variables and Tensorboard writer\n",
    "    tf.global_variables_initializer().run()\n",
    "    supervised_saver.restore(sess, \"./supervised/model.ckpt\")\n",
    "    guesses = sess.run([y_hat_super], feed_dict={x_super: data_test})[0]\n",
    "print(f'Accuracy of supervised model on test set: {accuracy(guesses, labels_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Semi-Supervised Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_semi = tf.Graph()\n",
    "with g_semi.as_default():\n",
    "    \n",
    "    # Set tf seed\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    # Inputs\n",
    "    x_semi = tf.placeholder(tf.float32, shape=[None, 30], name='x')\n",
    "    y_semi = tf.placeholder(tf.int32, shape=[None,], name='y')\n",
    "    eps = tf.placeholder(tf.float32, shape=[None, 10], name='eps')\n",
    "\n",
    "    # Unsupervised Model\n",
    "    with tf.variable_scope(\"unsupervised\"):\n",
    "        mu = dense(inputs=x_semi, activation='relu', units=10)\n",
    "        sigma = dense(inputs=x_semi, activation='relu', units=10)\n",
    "        z = mu + sigma * eps\n",
    "        x_hat = dense(dense(inputs=z, units=10), units=30)\n",
    "\n",
    "    # Supervised Model ON TOP of Unsupervised Latent Variables\n",
    "    with tf.variable_scope(\"supervised\"):\n",
    "        logits = dense(inputs=z, activation=None, units=2)\n",
    "        y_hat_semi = tf.argmax(logits, 1)\n",
    "        \n",
    "    # Unsupervised Loss\n",
    "    unsupervised_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"unsupervised\")\n",
    "    recon = tf.reduce_sum(tf.squared_difference(x_semi, x_hat))\n",
    "    vae = -0.5 * tf.reduce_sum(1.0 - tf.square(mu) - tf.square(sigma) + 2.0 * tf.log(sigma + 1e-8))\n",
    "    vae_cost = tf.reduce_sum(recon + 0.01 * vae)\n",
    "    vae_optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(vae_cost, var_list=unsupervised_vars)\n",
    "    \n",
    "    # Supervised Loss\n",
    "    supervised_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"supervised\")\n",
    "    super_cost = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=y_semi)\n",
    "    super_optimizer = tf.train.AdamOptimizer(learning_rate=8e-3).minimize(super_cost, var_list=supervised_vars)\n",
    "\n",
    "    # Summaries\n",
    "    tf.summary.scalar(\"Unsuper_Vae_loss\", vae)\n",
    "    tf.summary.scalar(\"Unsuper_Recon_loss\", recon)\n",
    "    tf.summary.scalar(\"Unsuper_Total_loss\", vae_cost)\n",
    "    tf.summary.scalar(\"Super_Total_loss\", super_cost)\n",
    "    merged_semi = tf.summary.merge_all()\n",
    "        \n",
    "    # Saver\n",
    "    semi_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Unsupervised Training on Semi-Supervised Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unsupervised model on train set at 0 iterations: 0.572\n",
      "Accuracy of unsupervised model on train set at 1000 iterations: 0.632\n",
      "Accuracy of unsupervised model on train set at 2000 iterations: 0.768\n",
      "Accuracy of unsupervised model on train set at 3000 iterations: 0.336\n",
      "Accuracy of unsupervised model on train set at 4000 iterations: 0.344\n",
      "Accuracy of unsupervised model on train set at 5000 iterations: 0.416\n",
      "Accuracy of unsupervised model on train set at 6000 iterations: 0.368\n",
      "Accuracy of unsupervised model on train set at 7000 iterations: 0.42\n",
      "Accuracy of unsupervised model on train set at 8000 iterations: 0.356\n",
      "Accuracy of unsupervised model on train set at 9000 iterations: 0.472\n",
      "Accuracy of unsupervised model on train set at 10000 iterations: 0.36\n",
      "Model saved in path: ./vae/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "with tf.Session(graph=g_semi) as sess:\n",
    "    # Initialize variables and Tensorboard writer\n",
    "    writer = tf.summary.FileWriter('./vae', g_semi)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(10001):\n",
    "        # Generate training batches\n",
    "        epsilon = np.random.normal(size=(250, 10))\n",
    "        indexes = np.random.randint(low=0, high=data_train.shape[0]-1, size=250)\n",
    "        feed_dict = {x_semi: data_train[indexes], y_semi: labels_train[indexes], eps: epsilon}\n",
    "\n",
    "        # Training iteration\n",
    "        summary, y_hatt, _ = sess.run([merged_semi, y_hat_semi, vae_optimizer], feed_dict=feed_dict)\n",
    "        if step % 1000 == 0:\n",
    "            print(f'Accuracy of unsupervised model on train set at {step} iterations: {accuracy(y_hatt, labels[indexes])}')\n",
    "        writer.add_summary(summary=summary, global_step=step)\n",
    "        \n",
    "    save_path = semi_saver.save(sess, \"./vae/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Supervised Training on Semi-Supervised Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vae/model.ckpt\n",
      "Accuracy of supervised model on train set at 0 iterations: 0.416\n",
      "Accuracy of supervised model on train set at 500 iterations: 0.944\n",
      "Accuracy of supervised model on train set at 1000 iterations: 0.94\n",
      "Accuracy of supervised model on train set at 1500 iterations: 0.908\n",
      "Accuracy of supervised model on train set at 2000 iterations: 0.96\n",
      "Accuracy of supervised model on train set at 2500 iterations: 0.92\n",
      "Model saved in path: ./semisupervised/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "with tf.Session(graph=g_semi) as sess:\n",
    "    # Initialize variables and Tensorboard writer\n",
    "    writer = tf.summary.FileWriter('./semisupervised', g_semi)\n",
    "    tf.global_variables_initializer().run()\n",
    "    semi_saver.restore(sess, \"./vae/model.ckpt\")\n",
    "\n",
    "    for step in range(2501):\n",
    "        # Generate training batches\n",
    "        epsilon = np.random.normal(size=(250, 10))\n",
    "        indexes = np.random.randint(low=0, high=data_train.shape[0]-1, size=250)\n",
    "        feed_dict = {x_semi: data_train[indexes], y_semi: labels_train[indexes], eps: epsilon}\n",
    "\n",
    "        # Training iteration\n",
    "        summary, y_hatt, _ = sess.run([merged_semi, y_hat_semi, super_optimizer], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(f'Accuracy of supervised model on train set at {step} iterations: {accuracy(y_hatt, labels[indexes])}')\n",
    "        writer.add_summary(summary=summary, global_step=step)\n",
    "    \n",
    "    save_path = semi_saver.save(sess, \"./semisupervised/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Trained Semi-Supervised Model on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./semisupervised/model2.ckpt\n",
      "Accuracy of semi-supervised model on test set: 0.9855072463768116\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g_semi) as sess:\n",
    "    # Initialize variables and Tensorboard writer\n",
    "    tf.global_variables_initializer().run()\n",
    "    semi_saver.restore(sess, \"./semisupervised/model2.ckpt\")\n",
    "    epsilon = np.random.normal(size=(data_test.shape[0], 10))\n",
    "    guesses = sess.run([y_hat_semi], feed_dict={x_semi: data_test, eps: epsilon})[0]\n",
    "print(f'Accuracy of semi-supervised model on test set: {accuracy(guesses, labels_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "columnbus",
   "language": "python",
   "name": "columnbus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
